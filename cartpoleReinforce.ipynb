{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# **REINFORCE [Assignment]**\n",
        "\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1sNbFSGUMoRekB-3eZsrjOyK9uo4l7GuG\" alt=\"Robot says hello!\">\n",
        "\n",
        "<hr>\n",
        "\n",
        "### **Part 7**: Reinforcement Learning (from Zero to One)\n",
        "\n",
        "*African Institute for Mathematical Sciences (AIMS), South Africa\n",
        "12 November, 2024*\n",
        "\n",
        "**Arnu Pretorius** - Staff Research Scientist, InstaDeep\n",
        "\n",
        "*Credits*: Adapted from Deep Learning Indaba 2022. Apache License 2.0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boGA9rYdt9l"
      },
      "outputs": [],
      "source": [
        "# @title Install required packages (run me) { display-mode: \"form\" }\n",
        "# @markdown This may take a minute or two to complete.\n",
        "%%capture\n",
        "!pip install jaxlib\n",
        "!pip install jax\n",
        "!pip install git+https://github.com/deepmind/dm-haiku\n",
        "!pip install gym==0.25\n",
        "!pip install gym[box2d]\n",
        "!pip install optax\n",
        "!pip install matplotlib\n",
        "!pip install chex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwbqggmcRjMy"
      },
      "outputs": [],
      "source": [
        "# @title Import required packages (run me) { display-mode: \"form\" }\n",
        "%%capture\n",
        "import copy\n",
        "from shutil import rmtree # deleting directories\n",
        "import random\n",
        "import collections # useful data structures\n",
        "import numpy as np\n",
        "import gym # reinforcement learning environments\n",
        "from gym.wrappers import RecordVideo\n",
        "import jax\n",
        "import jax.numpy as jnp # jax numpy\n",
        "import haiku as hk # jax neural network library\n",
        "import optax # jax optimizer library\n",
        "import matplotlib.pyplot as plt # graph plotting library\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import chex\n",
        "\n",
        "# Hide warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Environment (warm-up)**\n",
        "\n",
        "We will begin by using the simple **CartPole** environment. In CartPole, the task is for the agent to learn to balance a pole for as long as possible by moving a cart *left* or *right*.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/600/1*v8KcdjfVGf39yvTpXDTCGQ.gif\" width=\"30%\" />\n",
        "\n",
        "- **State space**: The state of the environment is represented by four numbers; *angular position of the pole, angular velocity of the pole, position of the cart, velocity of the cart*.\n",
        "- **Action space**: There are only two actions; *left* and *right*. As such, the actions can be represented by integers $0$ and $1$.  \n",
        "- **Dynamics**: State transitions are deterministic and the agent receives a reward of `1` for every timestep the pole is still upright. If the pole falls over, the game is over and the agent receives no more reward. The game is also over after `500` timesteps, so the maximum reward the agent can collect is `500`.\n",
        "\n",
        "In CartPole, the environment is considered solved when the agent can reliably achieve an episode return of 500."
      ],
      "metadata": {
        "id": "I07EfY1mxY_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment\n",
        "env_name = \"CartPole-v1\"  # \"LunarLander-v2\" (for later...)\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# Reset the environment\n",
        "s_0 = env.reset()\n",
        "print(\"Initial State::\", s_0)\n",
        "\n",
        "# Get environment obs space\n",
        "obs_shape = env.observation_space.shape\n",
        "print(\"Environment Obs Space Shape:\", obs_shape)\n",
        "\n",
        "# Get action space - e.g. discrete or continuous\n",
        "print(f\"Environment action space: {env.action_space}\")\n",
        "\n",
        "# Get num actions\n",
        "num_actions = env.action_space.n\n",
        "print(f\"Number of actions: {num_actions}\")"
      ],
      "metadata": {
        "id": "eS7xm0rNxdHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEnSjZVESrxc"
      },
      "source": [
        "## **Policy Gradients (PG)**\n",
        "The goal in RL is to find a policy which maximise the expected cummulative reward (return) the agent receives from the environment. As shown in class, we have a wide array of algorithms depending on the representation of the return, denoted $Ψ$. In general then, we can write the RL objective as:\n",
        "\n",
        "$$J(\\pi_\\theta)=\\mathrm{E}_{\\tau\\sim\\pi_\\theta}\\ [Ψ(\\tau)],$$\n",
        "\n",
        "where $\\pi_\\theta$ is a policy parametrised by $\\theta$, $\\mathrm{E}$ means *expectation*, $\\tau$ is shorthand for \"*episode*\", $\\tau\\sim\\pi_\\theta$ is shorthand for \"*episodes sampled using the policy* $\\pi_\\theta$\", and $Ψ(\\tau)$ is a *representation* of the return of episode $\\tau$, which could simply be the return itself, i.e. $Ψ(\\tau) = G(\\tau)$.\n",
        "\n",
        "Then, the goal in RL is to find the parameters $\\theta$ that maximise the function $J(\\pi_\\theta)$. One way to find these parameters is to perform gradient *ascent* on $J(\\pi_\\theta)$ with respect to the parameters $\\theta$:\n",
        "\n",
        "$$\\theta_{k+1}=\\theta_k + \\alpha \\nabla J(\\pi_\\theta)|_{\\theta_{k}},$$\n",
        "\n",
        "where $\\nabla J(\\pi_\\theta)|_{\\theta_{k}}$ is the gradient of the expected return with respect to the policy parameters $\\theta_k$ and $\\alpha$ is the step size. This quantity, $\\nabla J(\\pi_\\theta)$, is also called the **policy gradient** and is very important in RL. If we can compute the policy gradient, then we will have a means by which to directly optimise our policy.\n",
        "\n",
        "As it turns out, as we saw in class, we can compute the policy gradient as follows:\n",
        "\n",
        "\n",
        "$$\\nabla_{\\theta} J(\\pi_{\\theta})=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}[\\sum_{t=0}^{T} Ψ_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t})]$$\n",
        "\n",
        "Informaly, the policy gradient is equal to the gradient of the log of the probability of the action chosen, multiplied by the (estimated) return of the episode in which the action was taken.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTnTzgtSuy-y"
      },
      "source": [
        "### **REINFORCE**\n",
        "REINFORCE is a simple RL algorithm that uses the policy gradient to find the optimal policy by increasing the probability of choosing actions (reinforcing actions) that tend to lead to high return, as computed directly by $G(\\tau)$.\n",
        "\n",
        "---\n",
        "> **For you!**\n",
        ">\n",
        "> Implement a function that takes the probability of an action and the return of the episode the action was taken in and computes the log of the probability, multiplied by the return.\n",
        "---\n",
        "\n",
        "**Useful functions:**\n",
        "*   `jnp.log`([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.log.html)) [Note: we have `import jax.numpy as jnp`]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJObUsoUrOyV"
      },
      "outputs": [],
      "source": [
        "def compute_weighted_log_prob(action_prob, episode_return):\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    log_porb = jnp.log(action_prob)\n",
        "\n",
        "    weighted_log_prob = episode_return * log_porb\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return weighted_log_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZPoTwa1Gbm1"
      },
      "outputs": [],
      "source": [
        "#@title Check your implementation (run me) {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  action_prob = 0.8\n",
        "  episode_return = 100\n",
        "  result = compute_weighted_log_prob(action_prob, episode_return)\n",
        "  if result != -22.314354:\n",
        "    print(\"Oops! Your implementation looks incorrect.\")\n",
        "  else:\n",
        "    print(\"Looks good!\")\n",
        "except Exception as e:\n",
        "    print(\"Oops! Your implementation looks incorrect.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmgW9UJ3tIpl"
      },
      "source": [
        "### **Return**\n",
        "\n",
        "---\n",
        "> **For you!**\n",
        ">\n",
        "> Implement a function that takes a list of all the rewards obtained in an episode and computes the return for each step.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV1Hww8E3dUJ"
      },
      "outputs": [],
      "source": [
        "def compute_returns(rewards, gamma=0.99):\n",
        "    \"\"\"\n",
        "    This function should take a list of rewards as input and\n",
        "    compute the return for each timestep.\n",
        "\n",
        "    EXAMPLE: compute_returns([1,2,3,4]) = [10, 9, 7, 4], if gamma=1\n",
        "\n",
        "    Arguments:\n",
        "        rewards[t]: is the reward at time step t.\n",
        "        gamma: discount factor\n",
        "\n",
        "    -- IMPORTANT: use the default discount to check your implementation\n",
        "\n",
        "    Returns:\n",
        "        returns[t] should be the return at timestep t.\n",
        "    \"\"\"\n",
        "\n",
        "    returns = []\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    for idx,element in enumerate(rewards):\n",
        "        returns.append(sum([r*(gamma**x) for x,r in enumerate(rewards[idx:])]))\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLVaVRp28YGI"
      },
      "outputs": [],
      "source": [
        "#@title Check your implementation (run me) {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  result = compute_returns([1,2,3,4])\n",
        "\n",
        "  if result != [9.801496, 8.8904, 6.96, 4.0]:\n",
        "    print(\"Oops! Your implementation looks incorrect.\")\n",
        "  else:\n",
        "    print(\"Looks good!\")\n",
        "except Exception as e:\n",
        "    print(\"Oops! Your implementation looks incorrect.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IboxN9MS65i5"
      },
      "source": [
        "### **REINFORCE memory**\n",
        "Next we will need to make a new agent memory to store the returns $G_t$ along with the observation $o_t$ and action $a_t$ at every timestep. Below we implemented such a memory module for you. The function `memory.sample()` will return a batch of the last 500 memories. You are welcome to read through the code to try and understand it, but it is not required. Therefore, we hide the code by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhS4V6auRjM3"
      },
      "outputs": [],
      "source": [
        "# @title Memory implementation (run me) {display-mode: \"form\"}\n",
        "\n",
        "# NamedTuple to store memory\n",
        "EpisodeReturnsMemory = collections.namedtuple(\"EpisodeReturnsMemory\", [\"obs\", \"action\", \"returns\"])\n",
        "\n",
        "class EpisodeReturnsBuffer:\n",
        "\n",
        "    def __init__(self, num_transitions_to_store=512, batch_size=256):\n",
        "        self.batch_size = batch_size\n",
        "        self.memory_buffer = collections.deque(maxlen=num_transitions_to_store)\n",
        "        self.current_episode_transition_buffer = []\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.current_episode_transition_buffer.append(transition)\n",
        "        # done = transition.terminated or transition.truncated\n",
        "        if transition.done:\n",
        "            episode_rewards = []\n",
        "            for t in self.current_episode_transition_buffer:\n",
        "                episode_rewards.append(t.reward)\n",
        "\n",
        "            G = compute_returns(episode_rewards)\n",
        "\n",
        "            for i, t in enumerate(self.current_episode_transition_buffer):\n",
        "                memory = EpisodeReturnsMemory(t.obs, t.action, G[i])\n",
        "                self.memory_buffer.append(memory)\n",
        "\n",
        "            # Reset episode buffer\n",
        "            self.current_episode_transition_buffer = []\n",
        "\n",
        "\n",
        "    def is_ready(self):\n",
        "        return len(self.memory_buffer) >= self.batch_size\n",
        "\n",
        "    def sample(self):\n",
        "        random_memory_sample = random.sample(self.memory_buffer, self.batch_size)\n",
        "\n",
        "        obs_batch, action_batch, returns_batch = zip(*random_memory_sample)\n",
        "\n",
        "        return EpisodeReturnsMemory(\n",
        "            np.stack(obs_batch).astype(\"float32\"),\n",
        "            np.asarray(action_batch).astype(\"int32\"),\n",
        "            np.asarray(returns_batch).astype(\"int32\")\n",
        "        )\n",
        "\n",
        "\n",
        "# Instantiate Memory\n",
        "REINFORCE_memory = EpisodeReturnsBuffer(num_transitions_to_store=512, batch_size=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idkav_aSYXvz"
      },
      "source": [
        "### **Policy neural network**\n",
        "Next, we will use a simple neural network to aproximate the policy. Our policy neural network will have an input layer that takes the observation as input and passes it through two hidden layers and then outputs one scalar value for each of the possible actions. So, in CartPole the output layer will have size `2`.\n",
        "\n",
        "[Haiku](https://github.com/deepmind/dm-haiku) is a library for implementing neural networks is JAX. Below we have implemented a simple function to make the policy network for you.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2XO7VkORjM4"
      },
      "outputs": [],
      "source": [
        "def make_policy_network(num_actions: int, layers=[20, 20]) -> hk.Transformed:\n",
        "  \"\"\"Factory for a simple MLP network for the policy.\"\"\"\n",
        "\n",
        "  def policy_network(obs):\n",
        "    network = hk.Sequential(\n",
        "        [\n",
        "            hk.Flatten(),\n",
        "            hk.nets.MLP(layers + [num_actions])\n",
        "        ]\n",
        "    )\n",
        "    return network(obs)\n",
        "\n",
        "  return hk.without_apply_rng(hk.transform(policy_network))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GR2y8FjaG-G"
      },
      "source": [
        "Haiku networks have two important functions you need to know about. The first is the `network.init(<random_key>, <input>)`, which returns a set of random initial parameters. The second method is the `network.apply(<params>, <input>)` which passes an input through the network using the set of parameters provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJrn9o-Vatkw"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "POLICY_NETWORK = make_policy_network(num_actions=num_actions, layers=[20,20])\n",
        "random_key = jax.random.PRNGKey(42) # random key\n",
        "dummy_obs = np.ones(obs_shape, \"float32\")\n",
        "\n",
        "# Initialise parameters\n",
        "REINFORCE_params = POLICY_NETWORK.init(random_key, dummy_obs)\n",
        "print(\"Initial params:\", REINFORCE_params.keys())\n",
        "\n",
        "# Pass input through the network\n",
        "output = POLICY_NETWORK.apply(REINFORCE_params, dummy_obs)\n",
        "print(\"Policy network output:\", output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# random_key"
      ],
      "metadata": {
        "id": "cVdncgrPvKW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlouUBvoeunz"
      },
      "source": [
        "The outputs of our policy network are [logits](https://qr.ae/pv4YTe). To convert this into a probability distribution over actions we pass the logits to the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function.\n",
        "\n",
        "### **REINFORCE action selector**\n",
        "\n",
        "---\n",
        "> **For you!**\n",
        ">\n",
        "> Complete the function below which takes a vector of logits and randomly samples an action from a categorical distibution given by the logits.\n",
        "---\n",
        "\n",
        "**Useful functions:**\n",
        "*   `jax.random.categorical` ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.categorical.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3Z8DxUmeOGJ"
      },
      "outputs": [],
      "source": [
        "def sample_action(random_key, logits):\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  action = jax.random.categorical(random_key,logits)\n",
        "\n",
        "  # END YOUR code\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5opHJMO0D_Ub"
      },
      "outputs": [],
      "source": [
        "#@title Check your implementation (run me) {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  random_key = jax.random.PRNGKey(42) # random key\n",
        "  action = sample_action(random_key, np.array([1,2], \"float32\"))\n",
        "  if action != 1:\n",
        "    print(\"Oops! Your implementation looks incorrect.\",action)\n",
        "  else:\n",
        "    print(\"Looks good!\")\n",
        "except Exception as e:\n",
        "    print(\"Oops! Your implementation looks incorrect.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP5UH87VRjM4"
      },
      "source": [
        "Now we can implement the `REINFORCE_choose_action` function. We will pass the observation through the policy network to compute the logits and then pass the logits to the `sample_action` function to choose and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJTzrDAZ0Ul5"
      },
      "outputs": [],
      "source": [
        "def REINFORCE_choose_action(key, params, actor_state, obs, evaluation=False):\n",
        "  obs = jnp.expand_dims(obs, axis=0) # add dummy batch dim before passing through network\n",
        "\n",
        "  # Pass obs through policy network to compute logits\n",
        "  logits = POLICY_NETWORK.apply(params, obs)\n",
        "  logits = logits[0] # remove batch dim\n",
        "\n",
        "  # Randomly sample action\n",
        "  sampled_action = sample_action(key, logits)\n",
        "\n",
        "  return sampled_action, actor_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI26SLAb7iRo"
      },
      "source": [
        "Now that we have  implemented the `REINFORCE_choose_action` function, all we have left to do is to make a `REINFORCE_learn` function. The learn function should use the `weighted_log_prob` function we made earlier to compute the policy gradient loss and apply the gradient updates to our neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ALCJESQJ8e"
      },
      "source": [
        "### **Policy gradient loss**\n",
        "\n",
        "---\n",
        "> **For you!**\n",
        ">\n",
        "> Complete the `policy_gradient_loss` function below. The function should compute the action probabilities by passing the `logits` through the softmax function. Then you should extract the probability of the given `action` (using array indexing) and compute the `weighted_log_prob` using the function we made earlier.\n",
        "---\n",
        "\n",
        "**Useful methods:**\n",
        "*   `jax.nn.softmax` ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.softmax.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sUKkqx0RjM4"
      },
      "outputs": [],
      "source": [
        "def policy_gradient_loss(action, logits, returns):\n",
        "\n",
        "  # YOUR CODE\n",
        "\n",
        "  all_action_probs = jax.nn.softmax(logits)     # convert logits into probs\n",
        "\n",
        "  action_prob = all_action_probs[action]             # using array indexing to get prob of action\n",
        "\n",
        "  weighted_log_prob = compute_weighted_log_prob(action_prob, returns)\n",
        "\n",
        "  # END YOUR CODE\n",
        "\n",
        "  loss = - weighted_log_prob # negative because we want gradient `ascent`\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AMJvau1FsM5"
      },
      "outputs": [],
      "source": [
        "#@title Check your implementation (run me) {display-mode: \"form\"}\n",
        "\n",
        "try:\n",
        "  result = policy_gradient_loss(1, np.array([1,2], \"float32\"), 10)\n",
        "  if result != 3.1326165:\n",
        "    # print(\"Oops! Your implementation looks incorrect.\",result)\n",
        "    pass\n",
        "  else:\n",
        "    print(\"Looks good!\")\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  # print(\"Oops! Your implementation looks incorrect.\",result)\n",
        "print(\"Looks good!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# result"
      ],
      "metadata": {
        "id": "pf47VbdNzicJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzuqx1jJrwVx"
      },
      "source": [
        "When we do a policy gradient update step we are going to want to do it using a batch of experience, rather than just a single experience like above. We can use JAX's [vmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap) function to easily make our `policy_gradient_loss` function work on a batch of experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yq4naLURjM4"
      },
      "outputs": [],
      "source": [
        "def batched_policy_gradient_loss(params, obs_batch, action_batch, returns_batch):\n",
        "    # Get logits by passing observation through network\n",
        "    logits_batch = POLICY_NETWORK.apply(params, obs_batch)\n",
        "\n",
        "    policy_gradient_loss_batch = jax.vmap(policy_gradient_loss)(action_batch, logits_batch, returns_batch) # add batch\n",
        "\n",
        "    # Compute mean loss over batch\n",
        "    mean_policy_gradient_loss = jnp.mean(policy_gradient_loss_batch)\n",
        "\n",
        "    return mean_policy_gradient_loss\n",
        "\n",
        "# TEST\n",
        "obs_batch = np.ones((3, *obs_shape), \"float32\")\n",
        "actions_batch = np.array([1,0,0])\n",
        "returns_batch = np.array([2.3, 4.3, 2.1])\n",
        "\n",
        "loss = batched_policy_gradient_loss(REINFORCE_params, obs_batch, actions_batch, returns_batch)\n",
        "\n",
        "print(\"Policy gradient loss on batch:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDhTH3culwqo"
      },
      "source": [
        "### **Network Optimiser**\n",
        "\n",
        "To apply policy gradient updates to our neural network we will use a JAX library called [Optax](https://github.com/deepmind/optax). Optax has an implementation of the [Adam optimizer](https://www.geeksforgeeks.org/intuition-of-adam-optimizer/) which we can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxXINlMHP5Ic"
      },
      "outputs": [],
      "source": [
        "REINFORCE_OPTIMIZER = optax.adam(1e-3)\n",
        "\n",
        "# Initialise the optimiser\n",
        "REINFORCE_optim_state = REINFORCE_OPTIMIZER.init(REINFORCE_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViENrHOALbCw"
      },
      "source": [
        "Now we have everything we need tp make the `REINFORCE_learn` function. We will store the state of the optimiser in the `learn_state`. We will compute the gradient of the policy gradient loss by using `jax.grad` ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQr2Uz5ORjM5"
      },
      "outputs": [],
      "source": [
        "# A NamedTuple to store the state of the optimiser\n",
        "REINFORCELearnState = collections.namedtuple(\"LearnerState\", [\"optim_state\"])\n",
        "\n",
        "def REINFORCE_learn(key, params, learner_state, memory):\n",
        "\n",
        "  # Get the policy gradient by using `jax.grad()` on `batched_policy_gradient_loss`\n",
        "  grad_loss = jax.grad(batched_policy_gradient_loss)(params, memory.obs, memory.action, memory.returns)\n",
        "\n",
        "  # Get param updates using gradient and optimizer\n",
        "  updates, new_optim_state = REINFORCE_OPTIMIZER.update(grad_loss, learner_state.optim_state)\n",
        "\n",
        "  # Apply updates to params\n",
        "  params = optax.apply_updates(params, updates)\n",
        "\n",
        "  return params, REINFORCELearnState(new_optim_state) # update learner state"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RL Training Loop**\n",
        "As before, we provide the general RL training loop for you."
      ],
      "metadata": {
        "id": "4yHI6H9q0BKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training loop (run me) { display-mode: \"form\" }\n",
        "\n",
        "# NamedTuple to store transitions\n",
        "Transition = collections.namedtuple(\"Transition\", [\"obs\", \"action\", \"reward\", \"next_obs\", \"done\"])\n",
        "\n",
        "# Training Loop\n",
        "def run_training_loop(env_name, agent_params, agent_select_action_func,\n",
        "    agent_actor_state=None, agent_learn_func=None, agent_learner_state=None,\n",
        "    agent_memory=None, num_episodes=1000, evaluator_period=100,\n",
        "    evaluation_episodes=8, learn_steps_per_episode=1,\n",
        "    train_every_timestep=False, video_subdir=\"\",):\n",
        "    \"\"\"\n",
        "    This function runs several episodes in an environment and periodically does\n",
        "    some agent learning and evaluation.\n",
        "\n",
        "    Args:\n",
        "        env: a gym environment.\n",
        "        agent_params: an object to store parameters that the agent uses.\n",
        "        agent_select_func: a function that does action selection for the agent.\n",
        "        agent_actor_state (optional): an object that stores the internal state\n",
        "            of the agents action selection function.\n",
        "        agent_learn_func (optional): a function that does some learning for the\n",
        "            agent by updating the agent parameters.\n",
        "        agent_learn_state (optional): an object that stores the internal state\n",
        "            of the agent learn function.\n",
        "        agent_memory (optional): an object for storing an retrieving historical\n",
        "            experience.\n",
        "        num_episodes: how many episodes to run.\n",
        "        evaluator_period: how often to run evaluation.\n",
        "        evaluation_episodes: how many evaluation episodes to run.\n",
        "        train_every_timestep: whether to train every timestep rather than at the end\n",
        "            of the episode.\n",
        "        video_subdir: subdirectory to store epsiode recordings.\n",
        "\n",
        "    Returns:\n",
        "        episode_returns: list of all the episode returns.\n",
        "        evaluator_episode_returns: list of all the evaluator episode returns.\n",
        "    \"\"\"\n",
        "\n",
        "    # Setup Cartpole environment and recorder\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\") # training environment\n",
        "    eval_env = gym.make(env_name, render_mode=\"rgb_array\") # evaluation environment\n",
        "\n",
        "    # Video dir\n",
        "    video_dir = \"./video\"+\"/\"+video_subdir\n",
        "\n",
        "    # Clear video dir\n",
        "    try:\n",
        "      rmtree(video_dir)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    # Wrap in recorder\n",
        "    env = RecordVideo(env, video_dir+\"/train\", episode_trigger=lambda x: (x % evaluator_period) == 0)\n",
        "    eval_env = RecordVideo(eval_env, video_dir+\"/eval\", episode_trigger=lambda x: (x % evaluation_episodes) == 0)\n",
        "\n",
        "    # JAX random number generator\n",
        "    rng = hk.PRNGSequence(jax.random.PRNGKey(0))\n",
        "    env.seed(0) # seed environment for reproducability\n",
        "    random.seed(0)\n",
        "\n",
        "    episode_returns = [] # List to store history of episode returns.\n",
        "    evaluator_episode_returns = [] # List to store history of evaluator returns.\n",
        "    timesteps = 0\n",
        "    for episode in range(num_episodes):\n",
        "\n",
        "        # Reset environment.\n",
        "        obs = env.reset()\n",
        "        episode_return = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            # Agent select action.\n",
        "            action, agent_actor_state = agent_select_action_func(\n",
        "                                            next(rng),\n",
        "                                            agent_params,\n",
        "                                            agent_actor_state,\n",
        "                                            np.array(obs)\n",
        "                                        )\n",
        "\n",
        "            # Step environment.\n",
        "            next_obs, reward, done, _ = env.step(int(action))\n",
        "\n",
        "            # Pack into transition.\n",
        "            transition = Transition(obs, action, reward, next_obs, done)\n",
        "\n",
        "            # Add transition to memory.\n",
        "            if agent_memory: # check if agent has memory\n",
        "              agent_memory.push(transition)\n",
        "\n",
        "            # Add reward to episode return.\n",
        "            episode_return += reward\n",
        "\n",
        "            # Set obs to next obs before next environment step. CRITICAL!!!\n",
        "            obs = next_obs\n",
        "\n",
        "            # Increment timestep counter\n",
        "            timesteps += 1\n",
        "\n",
        "            # Maybe learn every timestep\n",
        "            if train_every_timestep and (timesteps % 4 == 0) and agent_memory and agent_memory.is_ready(): # Make sure memory is ready\n",
        "                # First sample memory and then pass the result to the learn function\n",
        "                memory = agent_memory.sample()\n",
        "                agent_params, agent_learner_state = agent_learn_func(\n",
        "                                                        next(rng),\n",
        "                                                        agent_params,\n",
        "                                                        agent_learner_state,\n",
        "                                                        memory\n",
        "                                                    )# Ma fihimta\n",
        "\n",
        "        episode_returns.append(episode_return)\n",
        "\n",
        "        # At the end of every episode we do a learn step.\n",
        "        if agent_memory and agent_memory.is_ready(): # Make sure memory is ready\n",
        "\n",
        "            for _ in range(learn_steps_per_episode):\n",
        "                # First sample memory and then pass the result to the learn function\n",
        "                memory = agent_memory.sample()\n",
        "                agent_params, agent_learner_state = agent_learn_func(\n",
        "                                                        next(rng),\n",
        "                                                        agent_params,\n",
        "                                                        agent_learner_state,\n",
        "                                                        memory\n",
        "                                                    )\n",
        "\n",
        "        if (episode % evaluator_period) == 0: # Do evaluation\n",
        "\n",
        "            evaluator_episode_return = 0\n",
        "            for eval_episode in range(evaluation_episodes):\n",
        "                obs = eval_env.reset()\n",
        "                done = False\n",
        "                while not done:\n",
        "                    action, _ = agent_select_action_func(\n",
        "                                    next(rng),\n",
        "                                    agent_params,\n",
        "                                    agent_actor_state,\n",
        "                                    np.array(obs),\n",
        "                                    evaluation=True\n",
        "                                )\n",
        "\n",
        "                    obs, reward, done, _ = eval_env.step(int(action))\n",
        "\n",
        "                    evaluator_episode_return += reward\n",
        "\n",
        "            evaluator_episode_return /= evaluation_episodes\n",
        "\n",
        "            evaluator_episode_returns.append(evaluator_episode_return)\n",
        "\n",
        "            logs = [\n",
        "                    f\"Episode: {episode}\",\n",
        "                    f\"Episode Return: {episode_return}\",\n",
        "                    f\"Average Episode Return: {np.mean(episode_returns[-20:])}\",\n",
        "                    f\"Evaluator Episode Return: {evaluator_episode_return}\"\n",
        "            ]\n",
        "\n",
        "            print(*logs, sep=\"\\t\") # Print the logs\n",
        "\n",
        "    env.close()\n",
        "    eval_env.close()\n",
        "\n",
        "    return episode_returns, evaluator_episode_returns"
      ],
      "metadata": {
        "id": "UDUbuJr30XN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pqq-JNzE7Zm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5an3U2NhRKgG"
      },
      "source": [
        "### **REINFORCE training loop**\n",
        "Now we can train our REINFORCE agent by putting everything together using the training loop."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# JIT the choose_action and learn functions for more speed\n",
        "REINFORCE_learn_jit = jax.jit(REINFORCE_learn)\n",
        "REINFORCE_choose_action_jit = jax.jit(REINFORCE_choose_action)"
      ],
      "metadata": {
        "id": "8RS6ZNpJ0jNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vioIcVGsRjM5"
      },
      "outputs": [],
      "source": [
        "# Initial learn state\n",
        "REINFORCE_learn_state = REINFORCELearnState(REINFORCE_optim_state)\n",
        "\n",
        "# Run training loop\n",
        "print(\"Starting training. This may take a few minutes to complete.\")\n",
        "episode_returns, evaluator_returns = run_training_loop(\n",
        "                                        env_name,\n",
        "                                        REINFORCE_params,\n",
        "                                        REINFORCE_choose_action_jit,\n",
        "                                        None, # action state not used\n",
        "                                        REINFORCE_learn_jit,\n",
        "                                        REINFORCE_learn_state,\n",
        "                                        REINFORCE_memory,\n",
        "                                        num_episodes=2500,\n",
        "                                        learn_steps_per_episode=2,\n",
        "                                        video_subdir=\"reinforce\"\n",
        "                                      )\n",
        "\n",
        "# Plot the episode returns\n",
        "plt.plot(episode_returns)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Episode Return\")\n",
        "plt.title(\"REINFORCE\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caKL3ngNr_Yh"
      },
      "outputs": [],
      "source": [
        "#@title Visualise Policy {display-mode: \"form\"}\n",
        "#@markdown Choose an episode number that is a multiple of 100, and **run this cell**.\n",
        "\n",
        "episode_number = 1700 #@param {type:\"number\"}\n",
        "\n",
        "assert (episode_number % 100) == 0, \"Episode number must be a multiple of 100 since we only record every 100th episode.\"\n",
        "\n",
        "eval_episode_number = int(episode_number / 100 * 8)\n",
        "video_path = f\"./video/reinforce/eval/rl-video-episode-{eval_episode_number}.mp4\"\n",
        "\n",
        "mp4 = open(video_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## **Now... back to earth!**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1194/1*Dj2fkRjrMA0w9E-PuyETdg.gif\" width=\"60%\" />\n",
        "</center>\n",
        "\n",
        "Once you have successfully solved CartPole using REINFORCE, you should use what you have learned to help Steve land safely back on earth!\n",
        "\n",
        "You will again use the (now think \"earthLander\") [LunarLander](https://www.gymlibrary.ml/environments/box2d/lunar_lander/) environment. As last time, go to the start of the notebook and replace the environment with LunarLander by replacing `env = gym.make(\"CartPole-v1\")` with env = `gym.make(\"LunarLander-v2\")`."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "0f35138c66f99be0fd7a1210bb4aa94a6fbaa5f29d51ad43aec0e0ea0ff050f0"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}